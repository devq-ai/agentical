# Comprehensive Alerting Rules for Agentical
# Combines Logfire alerts with Prometheus infrastructure alerts

# Logfire Alert Configuration (to be configured in Logfire dashboard)
logfire_alerts:
  # Application Performance Alerts
  - name: "High Error Rate"
    description: "Application error rate exceeds 5%"
    condition: |
      error_rate > 0.05
    window: "5m"
    severity: "critical"
    channels: ["slack", "email", "pagerduty"]
    runbook_url: "https://docs.agentical.com/runbooks/high-error-rate"
    
  - name: "Slow Response Time"
    description: "P95 response time exceeds 2 seconds"
    condition: |
      percentile(response_time, 95) > 2000
    window: "5m"
    severity: "warning"
    channels: ["slack"]
    runbook_url: "https://docs.agentical.com/runbooks/slow-response"
    
  - name: "Agent Task Failure Rate"
    description: "Agent task failure rate exceeds 10%"
    condition: |
      (agent_tasks_failed / agent_tasks_total) > 0.10
    window: "10m"
    severity: "critical"
    channels: ["slack", "email"]
    runbook_url: "https://docs.agentical.com/runbooks/agent-failures"
    
  - name: "Database Query Slow"
    description: "Average database query time exceeds 1 second"
    condition: |
      avg(database_query_duration_ms) > 1000
    window: "5m"
    severity: "warning"
    channels: ["slack"]
    runbook_url: "https://docs.agentical.com/runbooks/slow-database"
    
  - name: "Cache Miss Rate High"
    description: "Cache miss rate exceeds 50%"
    condition: |
      cache_miss_rate > 0.50
    window: "10m"
    severity: "warning"
    channels: ["slack"]
    runbook_url: "https://docs.agentical.com/runbooks/cache-performance"
    
  - name: "MCP Tool Failures"
    description: "MCP tool failure rate exceeds 5%"
    condition: |
      (mcp_tool_failures / mcp_tool_executions) > 0.05
    window: "5m"
    severity: "warning"
    channels: ["slack"]
    runbook_url: "https://docs.agentical.com/runbooks/mcp-tools"

# Prometheus Alert Rules for Infrastructure
prometheus_alerts:
  groups:
  - name: agentical.infrastructure
    rules:
    # Node-level alerts
    - alert: NodeCPUHigh
      expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
      for: 10m
      labels:
        severity: warning
        service: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is above 80% for more than 10 minutes on {{ $labels.instance }}"
        runbook_url: "https://docs.agentical.com/runbooks/high-cpu"
    
    - alert: NodeMemoryHigh
      expr: ((node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes) / node_memory_MemTotal_bytes) * 100 > 90
      for: 5m
      labels:
        severity: critical
        service: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is above 90% for more than 5 minutes on {{ $labels.instance }}"
        runbook_url: "https://docs.agentical.com/runbooks/high-memory"
    
    - alert: NodeDiskSpaceLow
      expr: ((node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"}) * 100 > 90
      for: 5m
      labels:
        severity: warning
        service: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is above 90% on {{ $labels.instance }} filesystem {{ $labels.mountpoint }}"
        runbook_url: "https://docs.agentical.com/runbooks/disk-space"
    
    # Kubernetes cluster alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"
        runbook_url: "https://docs.agentical.com/runbooks/pod-crashloop"
    
    - alert: PodNotReady
      expr: kube_pod_status_ready{condition="false"} == 1
      for: 10m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Pod {{ $labels.pod }} not ready"
        description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for more than 10 minutes"
        runbook_url: "https://docs.agentical.com/runbooks/pod-not-ready"
    
    - alert: DeploymentReplicasUnavailable
      expr: kube_deployment_status_replicas_unavailable > 0
      for: 5m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Deployment {{ $labels.deployment }} has unavailable replicas"
        description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} unavailable replicas"
        runbook_url: "https://docs.agentical.com/runbooks/deployment-replicas"
    
    # Application-specific infrastructure alerts
    - alert: AgenticalAPIDown
      expr: up{job="agentical-api"} == 0
      for: 1m
      labels:
        severity: critical
        service: agentical-api
      annotations:
        summary: "Agentical API is down"
        description: "Agentical API instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.agentical.com/runbooks/api-down"
    
    - alert: SurrealDBDown
      expr: up{job="surrealdb"} == 0
      for: 1m
      labels:
        severity: critical
        service: surrealdb
      annotations:
        summary: "SurrealDB is down"
        description: "SurrealDB instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.agentical.com/runbooks/surrealdb-down"
    
    - alert: RedisDown
      expr: up{job="redis"} == 0
      for: 1m
      labels:
        severity: critical
        service: redis
      annotations:
        summary: "Redis is down"
        description: "Redis instance {{ $labels.instance }} is down"
        runbook_url: "https://docs.agentical.com/runbooks/redis-down"
    
    # Database performance alerts
    - alert: DatabaseConnectionsHigh
      expr: agentical_database_connections{status="active"} / agentical_database_connections{status="max"} > 0.8
      for: 5m
      labels:
        severity: warning
        service: database
      annotations:
        summary: "High database connection usage"
        description: "Database connection pool {{ $labels.pool_name }} is using more than 80% of available connections"
        runbook_url: "https://docs.agentical.com/runbooks/db-connections"
    
    # Redis performance alerts
    - alert: RedisMemoryHigh
      expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
      for: 5m
      labels:
        severity: warning
        service: redis
      annotations:
        summary: "Redis memory usage high"
        description: "Redis instance {{ $labels.instance }} is using more than 90% of available memory"
        runbook_url: "https://docs.agentical.com/runbooks/redis-memory"
    
    - alert: RedisEvictionsHigh
      expr: rate(redis_evicted_keys_total[5m]) > 10
      for: 5m
      labels:
        severity: warning
        service: redis
      annotations:
        summary: "High Redis key evictions"
        description: "Redis instance {{ $labels.instance }} is evicting more than 10 keys per second"
        runbook_url: "https://docs.agentical.com/runbooks/redis-evictions"

# Alert routing configuration for Prometheus AlertManager
alertmanager_config:
  route:
    group_by: ['alertname', 'cluster', 'service']
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 12h
    receiver: 'default'
    routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 5m
      continue: true
    
    # Service-specific routing
    - match:
        service: agentical-api
      receiver: 'agentical-team'
      group_wait: 30s
      repeat_interval: 30m
    
    - match:
        service: infrastructure
      receiver: 'platform-team'
      group_wait: 2m
      repeat_interval: 1h
    
    - match:
        service: database
      receiver: 'database-team'
      group_wait: 1m
      repeat_interval: 30m

  receivers:
  - name: 'default'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#monitoring'
      title: 'Agentical Alert'
      text: |
        *Alert:* {{ .GroupLabels.alertname }}
        *Severity:* {{ .CommonLabels.severity }}
        *Service:* {{ .CommonLabels.service }}
        *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
      send_resolved: true

  - name: 'critical-alerts'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#critical-alerts'
      title: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
      text: |
        *Alert:* {{ .GroupLabels.alertname }}
        *Severity:* {{ .CommonLabels.severity }}
        *Service:* {{ .CommonLabels.service }}
        *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
        *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
        *Runbook:* {{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}
      send_resolved: true
      color: 'danger'
    
    # PagerDuty integration for critical alerts
    pagerduty_configs:
    - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
      description: 'Critical Alert: {{ .GroupLabels.alertname }}'
      severity: 'critical'
      details:
        summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        runbook_url: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
    
    # Email notifications for critical alerts
    email_configs:
    - to: 'oncall@agentical.com'
      from: 'alerts@agentical.com'
      subject: 'CRITICAL Alert: {{ .GroupLabels.alertname }}'
      body: |
        Alert: {{ .GroupLabels.alertname }}
        Severity: {{ .CommonLabels.severity }}
        Service: {{ .CommonLabels.service }}
        Summary: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
        Description: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
        
        Runbook: {{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}
        
        Time: {{ range .Alerts }}{{ .StartsAt }}{{ end }}

  - name: 'agentical-team'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#agentical-alerts'
      title: 'Agentical Service Alert: {{ .GroupLabels.alertname }}'
      text: |
        *Service:* {{ .CommonLabels.service }}
        *Alert:* {{ .GroupLabels.alertname }}
        *Severity:* {{ .CommonLabels.severity }}
        *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
      send_resolved: true

  - name: 'platform-team'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#platform-alerts'
      title: 'Infrastructure Alert: {{ .GroupLabels.alertname }}'
      text: |
        *Infrastructure Component:* {{ .CommonLabels.service }}
        *Alert:* {{ .GroupLabels.alertname }}
        *Severity:* {{ .CommonLabels.severity }}
        *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
      send_resolved: true

  - name: 'database-team'
    slack_configs:
    - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
      channel: '#database-alerts'
      title: 'Database Alert: {{ .GroupLabels.alertname }}'
      text: |
        *Database Service:* {{ .CommonLabels.service }}
        *Alert:* {{ .GroupLabels.alertname }}
        *Severity:* {{ .CommonLabels.severity }}
        *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
      send_resolved: true

# SLI/SLO Definitions
slo_definitions:
  api_availability:
    description: "API should be available 99.9% of the time"
    target: 0.999
    window: "30d"
    error_budget: 0.001
    
  api_latency:
    description: "95% of API requests should complete within 500ms"
    target: 0.95
    threshold: "500ms"
    window: "7d"
    
  agent_task_success:
    description: "95% of agent tasks should complete successfully"
    target: 0.95
    window: "24h"
    
  database_performance:
    description: "95% of database queries should complete within 100ms"
    target: 0.95
    threshold: "100ms"
    window: "24h"